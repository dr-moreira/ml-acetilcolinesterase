{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6386b936-7b24-4434-96bc-e5f29a1aac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3cfcdff-74b5-4333-aad4-67c34d5eb87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 13:11:38.167214: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-04 13:11:38.167266: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-04 13:11:38.167930: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-04 13:11:38.173423: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers, Model\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1456c6bf-b00a-4e45-abb8-9b7eb6e77596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f362026d-e12c-4e81-b66a-d8989e6f3e06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "IMG_SIZE = (480, 480)\n",
    "CLASS_NAMES = [ 'negativo', 'positivo']\n",
    "classes_names = np.array([ 'neg', 'pos'])\n",
    "train_path = pathlib.Path('./data/tmp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6ad6fca-6ae8-4d14-be39-c865e2e297ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positivo', 'negativo']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(tf.io.gfile.listdir(train_path))\n",
    "print(len(tf.io.gfile.listdir(train_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e842d6-5e9d-4b9c-a875-5247c75dd30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 13:11:39.993949: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-04 13:11:39.998752: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-04 13:11:39.998789: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-04 13:11:40.001410: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-04 13:11:40.001460: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-04 13:11:40.001486: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-04 13:11:40.103356: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-04 13:11:40.103406: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-04 13:11:40.103413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-02-04 13:11:40.103445: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:02:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-04 13:11:40.103462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5577 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:02:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'data/tmp/positivo/09 - pos_lam 897.jpg'\n",
      "b'data/tmp/positivo/68-pos_lam 933.jpg'\n",
      "b'data/tmp/positivo/1101.5_pos.jpg'\n",
      "b'data/tmp/negativo/64-neg_lam 953.jpg'\n",
      "b'data/tmp/positivo/1093.5_pos.jpg'\n"
     ]
    }
   ],
   "source": [
    "list_ds = tf.data.Dataset.list_files(str(train_path/'*/*'))\n",
    "\n",
    "for f in list_ds.take(5):\n",
    "  print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2759a5a-e81a-4f4b-b116-d0de3eac9a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads an image from a file, decodes it into a dense tensor, and resizes it\n",
    "# to a fixed shape.\n",
    "def parse_image(filename):\n",
    "  label = tf.strings.split(filename, os.sep)[-2]\n",
    "  if label == 'positivo':\n",
    "      label = 1.0\n",
    "  else: \n",
    "      label = 0.0\n",
    "  image = tf.io.read_file(filename)\n",
    "  image = tf.io.decode_jpeg(image)\n",
    "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "  image = tf.image.resize(image, IMG_SIZE)\n",
    "  return image, label\n",
    "    \n",
    "labeled_ds = list_ds.map(parse_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c85939-023a-4dc8-86fc-8c98241b369c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = np.stack(list(labeled_ds.map(lambda x,y: x)))\n",
    "targets = np.stack(list(labeled_ds.map(lambda x,y: y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64c0dd36-3c47-4e4a-8782-529773a64dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 3\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2462f8-de96-4bed-9a9f-3de458492e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(480,480,3,)))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(256, activation='relu'))\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  model.add(Dense(units=1, activation='sigmoid'))\n",
    "  return model, model.get_weights()\n",
    "\n",
    "def init_weights(same_model, weights):\n",
    "  same_model.set_weights(weights)\n",
    "\n",
    "model, weights = create_model()\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "\n",
    "  # Define the model architecture\n",
    "  init_weights(model, weights)\n",
    "\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(loss=loss_function,\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "  # Generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  # Fit data to model\n",
    "  history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=20,\n",
    "              verbose=2)\n",
    "\n",
    "  # Generate generalization metrics\n",
    "  scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "  acc_per_fold.append(scores[1] * 100)\n",
    "  loss_per_fold.append(scores[0])\n",
    "\n",
    "  # Increase fold number\n",
    "  fold_no = fold_no + 1\n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842097d-bd70-4a96-a519-14ee52814cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge inputs and targets\n",
    "# inputs = tf.concat((input_train, input_test), axis=0)\n",
    "# targets = tf.concat((target_train, target_test), axis=0)\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "\n",
    "  # Define the model architecture\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(480,480,3,)))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(256, activation='relu'))\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(loss=loss_function,\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "\n",
    "  # Generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  # Fit data to model\n",
    "  history = model.fit(inputs[train], targets[train],\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=20,\n",
    "              verbose=2)\n",
    "\n",
    "  # Generate generalization metrics\n",
    "  scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "  acc_per_fold.append(scores[1] * 100)\n",
    "  loss_per_fold.append(scores[0])\n",
    "\n",
    "  # Increase fold number\n",
    "  fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "339f10f9-a478-47a1-ac0c-b681f0011b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 13:16:04.932302: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at take_dataset_op.cc:227 : INVALID_ARGUMENT: count must be a scalar\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__TakeDataset_device_/job:localhost/replica:0/task:0/device:CPU:0}} count must be a scalar [Op:TakeDataset] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m test_indices \u001b[38;5;241m=\u001b[39m fold_indices[i]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Create a dataset for the test data\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mlabeled_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Create a dataset for the training data\u001b[39;00m\n\u001b[1;32m     21\u001b[0m train_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msetdiff1d(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(labeled_ds)), test_indices)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py:1561\u001b[0m, in \u001b[0;36mDatasetV2.take\u001b[0;34m(self, count, name)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# take_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m take_op\n\u001b[0;32m-> 1561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtake_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/take_op.py:24\u001b[0m, in \u001b[0;36m_take\u001b[0;34m(self, count, name)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take\u001b[39m(\u001b[38;5;28mself\u001b[39m, count, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# pylint: disable=unused-private-name\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TakeDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/take_op.py:35\u001b[0m, in \u001b[0;36m_TakeDataset.__init__\u001b[0;34m(self, input_dataset, count, name)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(count, dtype\u001b[38;5;241m=\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mint64, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m---> 35\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_common_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_dataset_ops.py:7544\u001b[0m, in \u001b[0;36mtake_dataset\u001b[0;34m(input_dataset, count, output_types, output_shapes, metadata, name)\u001b[0m\n\u001b[1;32m   7542\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   7543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 7544\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7545\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   7546\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__TakeDataset_device_/job:localhost/replica:0/task:0/device:CPU:0}} count must be a scalar [Op:TakeDataset] name: "
     ]
    }
   ],
   "source": [
    "# prompt: create an input pipeline that implements kfold crossvalidation using a tensorflow dataset of images and labels\n",
    "\n",
    "# Define the number of folds\n",
    "num_folds = 5\n",
    "\n",
    "# Create a list of indices for each fold\n",
    "fold_indices = []\n",
    "for i in range(num_folds):\n",
    "  fold_indices.append(np.arange(i * len(labeled_ds) // num_folds, (i + 1) * len(labeled_ds) // num_folds))\n",
    "\n",
    "# Create a list of tf.data.Datasets for each fold\n",
    "fold_datasets = []\n",
    "for i in range(num_folds):\n",
    "  # Get the indices for the current fold\n",
    "  test_indices = fold_indices[i]\n",
    "\n",
    "  # Create a dataset for the test data\n",
    "  test_dataset = labeled_ds.take(test_indices)\n",
    "\n",
    "  # Create a dataset for the training data\n",
    "  train_indices = np.setdiff1d(np.arange(len(labeled_ds)), test_indices)\n",
    "  train_dataset = labeled_ds.skip(test_indices).take(train_indices)\n",
    "\n",
    "  # Add the datasets to the list\n",
    "  fold_datasets.append((train_dataset, test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35e60d8a-43cc-4c0e-a7a9-0cd450f95f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5*55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d78c275-bb14-4607-a859-74be121b4329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "58/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4d4fc-c712-4605-bc5c-fcb092c5ce01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
